{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Algoritmos de clasificación de ajuste fino\n",
        "\n",
        "Este capítulo lo ayudará a optimizar el análisis predictivo mediante la clasificación algoritmos como máquinas de vectores de soporte, árboles de decisión y bosques aleatorios, que son algunos de los algoritmos de clasificación más comunes de la biblioteca de aprendizaje automático de scikit-learn. \n",
        "\n",
        "Además, aprenderá a implementar modelos de clasificación basados en árboles, que ha utilizado anteriormente para la regresión. A continuación, aprenderá a elegir las métricas de rendimiento adecuadas para evaluar el rendimiento de un modelo de clasificación. Finalmente, utilizará todas estas habilidades para resolver un problema de predicción de abandono de clientes en el que optimizará y evaluará el mejor algoritmo de clasificación para predecir si un cliente determinado abandonará o no.\n",
        "\n"
      ],
      "metadata": {
        "id": "jsr8KhNkTAxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intro\n",
        "\n",
        "Considere un escenario en el que usted es el líder de ML en una empresa de análisis de marketing. Su empresa se hizo cargo de un proyecto de Amazon para predecir si un usuario comprará o no un producto durante las campañas de venta de temporada festiva. Se le han proporcionado datos anónimos sobre la actividad del cliente en el sitio web de Amazon: la cantidad de productos comprados, sus precios, categorías de productos y más. En tales escenarios, donde la variable objetivo es un valor discreto, por ejemplo, el cliente comprará el producto o no, este tipo se denominan problemas de clasificación. \n",
        "\n",
        "Hay una gran cantidad de algoritmos de clasificación disponibles ahora para resolver tales problemas y elegir el correcto es una tarea crucial. Entonces, primero comenzará a explorar el conjunto de datos para obtener algunas observaciones al respecto. A continuación,\n",
        "- probará diferentes algoritmos de clasificación y evaluará las métricas de rendimiento de cada modelo de clasificación para comprender si el modelo es lo suficientemente bueno para ser utilizado por la compañía. \n",
        "- Finalmente, obtendrá el mejor algoritmo de clasificación de todo el grupo de modelos que entrenó, y este modelo se utilizará para predecir si un usuario comprará un producto durante la venta."
      ],
      "metadata": {
        "id": "CpBL2arsU6hV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Máquinas de soporte de vectores \n",
        "Cuando se trata de datos que son linealmente separables, el objetivo del algoritmo de aprendizaje de la Máquina de vectores de soporte (SVM) es encontrar el límite entre las clases para que haya menos errores de clasificación. Sin embargo, el problema es que puede haber varios límites de decisión (B1, B2), como se puede ver en la siguiente figura:\n",
        "\n",
        "https://blog.bismart.com/la-clasificaci%C3%B3n-y-la-clusterizaci%C3%B3n-una-explicaci%C3%B3n-pr%C3%A1ctica\n",
        "\n",
        "\n",
        "<figure>\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=18K3T0JH4UUWzev9QK6tuudPqrwqutwVg' width=\"300\" />\n",
        "<figcaption>\n",
        "\n",
        "Figura 8.1: Límite de decisión múltiple</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "Como resultado, surge la pregunta de cuál de los límites es mejor y cómo definir mejor. La solución es utilizar un margen como objetivo de optimización. Un margen se puede describir como la distancia entre el límite y dos puntos (de diferentes clases) que se encuentran más cerca del límite. La Figura 8.2 da una buena definición visual del margen.\n",
        "\n",
        "El objetivo del algoritmo SVM es maximizar el margen. Repasará la intuición detrás de maximizar el margen en la siguiente sección. Por ahora, debe comprender que el objetivo de un clasificador lineal SVM es aumentar el ancho del límite antes de llegar a un punto de datos. El algoritmo primero averigua el ancho del hiperplano y luego maximiza el margen. Elige el límite de decisión que tiene el margen máximo.\n",
        "\n",
        "\n",
        "Si bien puede parecer demasiado desalentador al principio, no necesita preocuparse por esto, ya que el algoritmo llevará a cabo internamente todas estas tareas y podrá brindarle la clase objetivo para un punto de datos determinado. Por ejemplo, en la figura anterior, elige $B1$ porque tenía un margen mayor en comparación con $B2$. Puede consultar los márgenes de los límites de decisión de $B1$ y $B2$ en la Figura 8.2:"
      ],
      "metadata": {
        "id": "DVWXJsEaVNd7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1hVGQlaWkek-DKg_kypCgJuZ_5BuR5dY3' width=\"300\" />\n",
        "<figcaption>\n",
        "\n",
        "Figura 8.2: Límite de decisión con un ancho/margen diferente</figcaption></center>\n",
        "</figure>\n",
        "\n"
      ],
      "metadata": {
        "id": "XbhJM2Gx0hXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ventajas\n",
        "- Las SVM son efectivas cuando se trata de datos de alta dimensión, donde el número de dimensiones es mayor que el número de muestras de entrenamiento.\n",
        "- Las SVM son conocidas por su uso de la función kernel, lo que la convierte en un algoritmo muy versátil.\n",
        "\n",
        "Nota: Los métodos kernel son funciones matemáticas que se utilizan para convertir datos de un espacio de menor dimensión a un espacio de mayor dimensión, o viceversa.\n",
        "\n",
        "Desventajas:\n",
        "- Las SVM no calculan la probabilidad directamente y, en su lugar, utilizan una validación cruzada quíntuple para calcular la probabilidad, lo que puede hacer que el algoritmo sea considerablemente lento.\n",
        "- Con datos de alta dimensión, es importante elegir la función kernel y el término de regularización, lo que puede hacer que el proceso sea muy lento."
      ],
      "metadata": {
        "id": "zkohy77G07jh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## La intuición detrás del Máximo Margen\n",
        "\n",
        "<figure>\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1cPt_hg1uBUukelapwldLZyvRvEwP-yMT' width=\"300\" />\n",
        "<figcaption>\n",
        "\n",
        "Figura 8.3: Interpretación geométrica del margen máximo</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "La lógica detrás de tener márgenes grandes en el caso de una SVM es que tienen un \n",
        "- error de generalización más bajo \n",
        "\n",
        "en comparación con los márgenes pequeños, lo que puede resultar en datos sobreajustados.\n",
        "\n",
        "Considere la Figura 8.3, donde tiene puntos de datos de dos clases: cuadrados y círculos. Los puntos de datos que están más cerca del límite se denominan \n",
        "- vectores de soporte, \n",
        "\n",
        "ya que se utilizan para calcular el margen. El margen del lado izquierdo del límite se \n",
        "- denomina hiperplano negativo \n",
        "- y el margen del lado derecho del límite se denomina hiperplano positivo.\n"
      ],
      "metadata": {
        "id": "z9otSOYT1nnO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Consideremos el hiperplano positivo y negativo de la siguiente manera:\n",
        "<h1><center>\n",
        "\n",
        "$w^{t}x_{pos} + b_0 = 1$ \n",
        "\n",
        "$w^{t}x_{neg} + b_0 = -1$\n",
        "\n",
        "Ecuación de hiperplano positivo y negativo\n",
        "</center></h1>"
      ],
      "metadata": {
        "id": "d4zIGBxE28Hv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En las ecuaciones anteriores:\n",
        "- $w$ se refiere a la pendiente del hiperplano.\n",
        "- $b0$ y $b1$ se refieren a las intersecciones de los hiperplanos.\n",
        "- $T$ se refiere a la transpuesta.\n",
        "- $x_pos$ y $x_neg$ se refieren a los puntos por los que pasan los hiperplanos positivo y negativo, respectivamente.\n",
        "\n",
        "Las ecuaciones anteriores también se pueden considerar como una ecuación de una línea:\n",
        "\n",
        "$y = mx + c$, donde $m$ es la pendiente y $c$ es la intersección. Debido a esta similitud, SVM se denomina clasificador lineal SVM.\n",
        "\n",
        "Restando las dos ecuaciones anteriores, se obtiene lo siguiente:\n",
        "\n",
        "<h1><center>\n",
        "\n",
        "$w^{t}(x_{pos}- x_{neg}) = 2$ \n",
        "\n",
        "\n",
        "Figura 8.5: Ecuación combinada de dos hiperplanos separados\n",
        "</center></h1>"
      ],
      "metadata": {
        "id": "iAWbZCH_4JgW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizando la ecuación por el vector $w$, se obtiene lo siguiente, donde $m$ se refiere a los puntos de datos que tienes e $i$ se refiere al $i$-ésimo punto de datos:\n",
        "\n",
        "<h1><center>\n",
        "\n",
        "$ \\big||w|\\big| = \t\\sqrt{ \\sum_{i=1}^{m} w_{i^2}}$\n",
        "\n",
        "Figura 8.6: Ecuación normalizada\n",
        "\n",
        "</center></h1>\n",
        "\n",
        "La ecuación anterior se reduce de la siguiente manera:\n",
        "\n",
        "<center>\n",
        "$ margin = \\frac{ w^{t}(x_{pos}- x_{neg})}{\\big||w|\\big|}$ \n",
        "$  = \\frac{2}{\\big||w|\\big|}$\n",
        "\n",
        "Figura 8.7: Ecuación para margen $m$\n",
        "\n",
        "</center>\n",
        "\n",
        "Ahora, la función objetivo se obtiene maximizando el margen dentro de la restricción de que el límite de decisión debería clasificar todos los puntos correctamente.\n",
        "\n",
        "Ahora, una vez que tenga listo el límite de decisión, puede usar la siguiente ecuación para clasificar los puntos según el lado del límite de decisión en el que se encuentran:\n",
        "\n",
        "Figura 8.8: Ecuación para separar los puntos de datos en un hiperplano\n",
        "Para implementar un clasificador basado en SVM, puede usar el módulo scikit-learn de la siguiente manera:"
      ],
      "metadata": {
        "id": "bsfWVpuB46q7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><center>\n",
        "\n",
        "$w^{t}x_{i} + b_0 \\geq 1 $   $ if y_i = 1$ \n",
        "\n",
        "$w^{t}x_{i} + b_0 \\leq -1 $   $ if y_i = -1$ \n",
        "\n",
        "Figura 8.8: Ecuación para separar los puntos de datos en un hiperplano\n",
        "</center></h1>"
      ],
      "metadata": {
        "id": "PNhgZLan7hkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para implementar un clasificador basado en SVM, puede usar el módulo scikit-learn de la siguiente manera:\n",
        "\n",
        "1. Importar svm desde scikit-learn:\n",
        "\n",
        "```\n",
        " from sklearn import svm\n",
        "```\n",
        "\n",
        "2. Cree una instancia del modelo SVM que luego se usará para entrenar en el conjunto de datos:\n",
        "\n",
        "   model = svm.SVC()\n",
        "\n",
        "  En la función anterior, también puede especificar el tipo de núcleo (lineal, sigmoide, rbf, etc.), el parámetro de regularización C, el valor gamma del núcleo, etc. Puede leer la lista completa de parámetros disponibles junto con sus valores predeterminados: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.\n",
        "\n",
        "3. Una vez que tenga la instancia del modelo, puede usar `model.fit(X_train, y_train)` para entrenar el modelo y `model.predict(X)` para obtener la predicción.\n",
        "\n",
        "Hasta ahora ha estado lidiando con el margen duro, que no deja espacio para errores. En otras palabras, todas las instancias de una clase deben estar en un lado del margen. Sin embargo, este comportamiento rígido puede afectar la generalización del modelo. Para resolver esto, puede usar un clasificador de \n",
        "- margen suave."
      ],
      "metadata": {
        "id": "BnOYpSLT8S0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Casos linealmente inseparables\n",
        "\n",
        "Con casos linealmente inseparables, como el que se ilustra en la siguiente figura, no puede usar un clasificador de margen rígido. La solución es introducir un nuevo tipo de clasificador, conocido como clasificador de margen suave, utilizando la variable de holgura ξ. La variable de holgura convierte las ecuaciones discutidas en la sección anterior en desigualdades al permitir algunos errores, como se muestra en la Figura 8.9:"
      ],
      "metadata": {
        "id": "zKtqeOzA9vKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1tacYCJHi8q-avC6oYJNygpfGuF-7MsgJ' width=\"300\" />\n",
        "<figcaption>\n",
        "\n",
        "Figura 8.9: Puntos de datos linealmente inseparables</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lqcTpRlF96VV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una SVM de margen suave funciona haciendo lo siguiente:\n",
        "1. Introducción a la variable de holgura\n",
        "2. Relajación de las restricciones\n",
        "3. Penalizar la relajación\n",
        "\n",
        "<figure>\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1mXt5qPxnxNihjAyo_7MSO7umYAvK3Di0' width=\"300\" />\n",
        "<figcaption>\n",
        "\n",
        "Figura 8.10: Uso de la variable de holgura ξ para datos linealmente inseparables</figcaption></center>\n",
        "</figure>\n"
      ],
      "metadata": {
        "id": "43TFH3f5-iny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las restricciones lineales se pueden cambiar agregando la variable de holgura a la ecuación en la Figura 8.5 de la siguiente manera:\n",
        "\n",
        "<h1><center>\n",
        "\n",
        "$w^{t}x_{i} + b_0 \\geq 1 -\\xi_i$   $ if y_i = 1$ \n",
        "\n",
        "$w^{t}x_{i} + b_0 \\leq -1 + \\xi_i $   $ if y_i = -1$ \n",
        "\n",
        "Figure 8.11: Linear constraints for maximizing margin with slack variable $\\xi$\n",
        "</center></h1>\n",
        "\n",
        "\n",
        "\n",
        "La función objetivo para puntos de datos linealmente inseparables se obtiene minimizando lo siguiente:\n",
        "\n",
        "<center>\n",
        "$ \\frac{1}{2}\\big||w|\\big|^2 +c \\big(\\sum_{i} \\xi_i\\big)$\n",
        "\n",
        "Figura 8.12: Función objetivo a minimizar\n",
        "\n",
        "</center>\n",
        "\n",
        "\n",
        "Aquí, $C$ es el parámetro del costo de la penalización (regularización). Este parámetro $C$ se puede especificar como un parámetro al llamar a la función `svm.SVC()`, como se explicó en la sección anterior.\n",
        "\n"
      ],
      "metadata": {
        "id": "pCdrpJ6E_Jvu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Casos linealmente inseparables usando el kernel\n",
        "En el ejemplo anterior, vio cómo puede usar una SVM de margen suave para clasificar conjuntos de datos usando la variable de holgura. Sin embargo, puede haber escenarios en los que sea bastante difícil separar los datos. Por ejemplo, en la siguiente figura, sería imposible tener un límite de decisión utilizando la variable de holgura y un hiperplano lineal:"
      ],
      "metadata": {
        "id": "6Qe8J9iJA-GH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<figure>\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=17q78lYjS99MWSpqkFq5lKWc8JuPHeS--' width=\"300\" />\n",
        "<figcaption>\n",
        "\n",
        "\n",
        "Figura 8.13: Puntos de datos linealmente inseparables</figcaption></center>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "WpROFB8RBJPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este escenario, puede utilizar el concepto de núcleo, que crea una combinación no lineal de características originales (X1, X2) para proyectar a un espacio de mayor dimensión a través de una función de mapeo, φ, para hacerlo linealmente separable:"
      ],
      "metadata": {
        "id": "TbNrg4_BBdFT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<figure>\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1DxNzquMAQ9k5xHynhLqDsYJ5TEg6Etde' width=\"500\" />\n",
        "<figcaption>\n",
        "\n",
        "\n",
        "Figura 8.14: Interpretación geométrica y ecuación para proyección de una dimensión baja a una alta</figcaption></center>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "BBBphAqqBm2A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El problema con este mapeo explícito de funciones es que la dimensionalidad de la función puede ser muy alta, lo que dificulta su representación explícita en la memoria. Esto se mitiga usando el truco del núcleo. El truco del kernel básicamente reemplaza el producto escalar xiT xj con un kernel φ xiTφ(xj), que se puede definir de la siguiente manera:\n",
        "\n",
        "<center>\n",
        "$ \\frac{1}{2}\\big||w|\\big|^2 +c \\big(\\sum_{i} \\xi_i\\big)$\n",
        "\n",
        "Figura 8.15: Función del núcleo\n",
        "\n",
        "</center>"
      ],
      "metadata": {
        "id": "bkfmtcJFB_Mj"
      }
    }
  ]
}